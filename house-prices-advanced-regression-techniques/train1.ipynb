{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import impyute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
       "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
       "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
       "       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
       "       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
       "       'MiscVal', 'MoSold', 'YrSold', 'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=data.describe()\n",
    "t=p.columns['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
    "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
    "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
    "       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
    "       'MiscVal', 'MoSold', 'YrSold', 'SalePrice']\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n",
    "       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n",
    "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
    "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
    "       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
    "       'MiscVal', 'MoSold', 'YrSold', 'SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 0\n",
       "MSSubClass         0\n",
       "LotFrontage      259\n",
       "LotArea            0\n",
       "OverallQual        0\n",
       "OverallCond        0\n",
       "YearBuilt          0\n",
       "YearRemodAdd       0\n",
       "MasVnrArea         8\n",
       "BsmtFinSF1         0\n",
       "BsmtFinSF2         0\n",
       "BsmtUnfSF          0\n",
       "TotalBsmtSF        0\n",
       "1stFlrSF           0\n",
       "2ndFlrSF           0\n",
       "LowQualFinSF       0\n",
       "GrLivArea          0\n",
       "BsmtFullBath       0\n",
       "BsmtHalfBath       0\n",
       "FullBath           0\n",
       "HalfBath           0\n",
       "BedroomAbvGr       0\n",
       "KitchenAbvGr       0\n",
       "TotRmsAbvGrd       0\n",
       "Fireplaces         0\n",
       "GarageYrBlt       81\n",
       "GarageCars         0\n",
       "GarageArea         0\n",
       "WoodDeckSF         0\n",
       "OpenPorchSF        0\n",
       "EnclosedPorch      0\n",
       "3SsnPorch          0\n",
       "ScreenPorch        0\n",
       "PoolArea           0\n",
       "MiscVal            0\n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SalePrice          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data=data[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>69.996159</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.765731</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>22.114994</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>180.589357</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.292500</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>165.250000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1460.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    69.996159   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    22.114994    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    60.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.292500    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    79.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000  ...   \n",
       "mean      5.575342  1971.267808   1984.865753   103.765731   443.639726  ...   \n",
       "std       1.112799    30.202904     20.645407   180.589357   456.098091  ...   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
       "75%       6.000000  2000.000000   2004.000000   165.250000   712.250000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
       "\n",
       "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
       "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
       "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
       "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
       "\n",
       "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
       "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
       "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
       "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
       "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
       "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
       "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
       "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
       "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "p=t\n",
    "\n",
    "subdata_in = data.loc[:,p]\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=400)\n",
    "subData=imputer.fit_transform(subdata_in)\n",
    "print(type(subData))\n",
    "\n",
    "D=dict()\n",
    "for i in range (len(t)):\n",
    "    D[t[i]]=subData[:, i]\n",
    "\n",
    "filledData = pd.DataFrame(D)\n",
    "\n",
    "filledData.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "\n",
    "def models(X_train,Y_train):\n",
    "    \n",
    "    #Use Logistic Regression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    log = LogisticRegression(random_state = 0,solver='lbfgs')\n",
    "    log.fit(X_train,Y_train)\n",
    "    \n",
    "    #Use Kneighbors\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn=KNeighborsClassifier(n_neighbors = 5,metric=\"minkowski\",p=2)\n",
    "    knn.fit(X_train,Y_train)\n",
    "    \n",
    "    #Use SVC (linear kernel)\n",
    "    from sklearn.svm import SVC\n",
    "    svc_lin=SVC(kernel=\"linear\",random_state=0)\n",
    "    svc_lin.fit(X_train,Y_train)\n",
    "    \n",
    "    #Use SVC (RBF kernel)\n",
    "    from sklearn.svm import SVC\n",
    "    svc_rbf=SVC(kernel=\"rbf\",gamma='auto',random_state=0)\n",
    "    svc_rbf.fit(X_train,Y_train)\n",
    "    \n",
    "    #Use GaussianNB\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    gauss = GaussianNB()\n",
    "    gauss.fit(X_train,Y_train)\n",
    "    \n",
    "    #Use Decision Tree\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    tree = DecisionTreeClassifier( criterion=\"entropy\" ,random_state=0)\n",
    "    tree.fit(X_train,Y_train)\n",
    "    \n",
    "    #Use Random Forest Classifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    forest = RandomForestClassifier(n_estimators=100,criterion=\"entropy\",random_state=0)\n",
    "    forest.fit(X_train,Y_train)\n",
    "    \n",
    "    #Use XGBClassifier\n",
    "    import xgboost as xgb\n",
    "    modelx=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "    modelx.fit(X_train,Y_train)\n",
    "    \n",
    "    #Use ExtraTreesClassifier\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    ExtraTree=ExtraTreesClassifier(n_estimators=100, max_depth=None,min_samples_split=2, random_state=8)\n",
    "    ExtraTree.fit(X_train,Y_train)\n",
    "    \n",
    "    #use AdaBoostClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    AdaBoost = AdaBoostClassifier(n_estimators=100)\n",
    "    AdaBoost.fit(X_train,Y_train)\n",
    "    \n",
    "    #print the accuracy of each model\n",
    "    print(\"[0] Logistic Regression Training Accuracy: \",log.score(X_train,Y_train))\n",
    "    print(\"[1] K Neighbors Regression Training Accuracy: \",knn.score(X_train,Y_train))\n",
    "    print(\"[2] SVC Linear Training Accuracy: \",svc_lin.score(X_train,Y_train))\n",
    "    print(\"[3] SVC RBF Training Accuracy: \",svc_rbf.score(X_train,Y_train))\n",
    "    print(\"[4] Gaussian NB Training Accuracy: \",gauss.score(X_train,Y_train))\n",
    "    print(\"[5] Decision Tree Training Accuracy: \",tree.score(X_train,Y_train))\n",
    "    print(\"[6] Random Forest Training Accuracy: \",forest.score(X_train,Y_train))\n",
    "    print(\"[7] XGBClassifier Training Accuracy: \",modelx.score(X_train,Y_train))\n",
    "    print(\"[8] ExtraTreesClassifier Training Accuracy: \",ExtraTree.score(X_train,Y_train))\n",
    "    print(\"[9] AdaBoostClassifierr Training Accuracy: \",AdaBoost.score(X_train,Y_train))\n",
    "    \n",
    "    \n",
    "    return  log,knn, svc_lin, svc_rbf, gauss, tree, forest,modelx,ExtraTree,AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================================================\n",
      "Only considering ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'] \n",
      "size = 1460         Train and test set 70% : 30%\n",
      "============================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Imesh Ekanayake\\.conda\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Logistic Regression Training Accuracy:  0.12426614481409001\n",
      "[1] K Neighbors Regression Training Accuracy:  0.1908023483365949\n",
      "[2] SVC Linear Training Accuracy:  1.0\n",
      "[3] SVC RBF Training Accuracy:  1.0\n",
      "[4] Gaussian NB Training Accuracy:  0.7573385518590998\n",
      "[5] Decision Tree Training Accuracy:  1.0\n",
      "[6] Random Forest Training Accuracy:  1.0\n",
      "[7] XGBClassifier Training Accuracy:  0.15362035225048923\n",
      "[8] ExtraTreesClassifier Training Accuracy:  1.0\n",
      "[9] AdaBoostClassifierr Training Accuracy:  0.021526418786692758\n",
      "\n",
      "============================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ca85f5748708>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m#Extract  TN, FP ,FN, TP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mTN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFP\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mtest_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mFP\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mTP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "p=t[:-1]\n",
    "\n",
    "subdata = filledData.loc[:,p+[t[-1]]]   \n",
    "#subdata.fillna(subdata.mean(), inplace=True)\n",
    "subData=subdata.dropna(subset=p)\n",
    "\n",
    "X=subData.loc[:,p]\n",
    "Y=subData.loc[:,[t[-1]]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train and test set 70% : 30%\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=3)\n",
    "Y_train = np.array(np.ravel(Y_train)).astype(int)\n",
    "\n",
    "\n",
    "print(\"\\n============================================================================================\")\n",
    "print(\"Only considering\",p,\"\\nsize =\",len(subData),\"        Train and test set 70% : 30%\")\n",
    "print(\"============================================================================================\\n\\n\")\n",
    "\n",
    "\n",
    "model = models(X_train,Y_train)\n",
    "ms=[\"Logistic Regression\",\"K Neighbors\",\"SVC Linear\",\"SVC RBF\",\"Gaussian NB\",\"Decision Tree\",\"Random Forest\",\n",
    "    \"XGBClassifier\",\"ExtraTreesClassifier\",\"AdaBoostClassifier\"]\n",
    "\n",
    "print(\"\\n============================================================================================\\n\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "for i in range (len(model)):\n",
    "    cm= confusion_matrix(Y_test,model[i].predict(X_test))\n",
    "\n",
    "    #Extract  TN, FP ,FN, TP\n",
    "    TN, FP ,FN, TP =  cm.ravel()\n",
    "    \n",
    "    test_score=(TP+TN)/(TN+ FP +FN+ TP)\n",
    "    \n",
    "\n",
    "    print(ms[i],'Testing Accuracy = ',test_score,\"\\n\")\n",
    "    print(\"\\tTP:\",TP,\"\\tFP:\",FP,\"\\n\\tTN:\",TN,\"\\tFN:\",FN )\n",
    "    print(\"\\n\"*2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "cm= confusion_matrix(Y_test,model[0].predict(X_test))\n",
    "p=cm.ravel()\n",
    "#Extract  TN, FP ,FN, TP\n",
    "print(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN = keras.Sequential()\n",
    "modelNN.add(layers.Dense(100, activation='relu',input_shape=(37,)))\n",
    "modelNN.add(layers.Dense(80, activation='relu'))\n",
    "modelNN.add(layers.Dense(40, activation='relu'))\n",
    "modelNN.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "modelNN.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 37)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 817 samples, validate on 205 samples\n",
      "Epoch 1/100\n",
      "817/817 [==============================] - 1s 967us/step - loss: 22417741710.4137 - acc: 0.0000e+00 - val_loss: 6272224846.0488 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "817/817 [==============================] - 0s 518us/step - loss: 7607293581.3170 - acc: 0.0000e+00 - val_loss: 3172923204.6829 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "817/817 [==============================] - 0s 521us/step - loss: 5105540374.7173 - acc: 0.0000e+00 - val_loss: 2608415603.5122 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "817/817 [==============================] - 0s 500us/step - loss: 4179782623.5692 - acc: 0.0000e+00 - val_loss: 2254932195.9024 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "817/817 [==============================] - 0s 549us/step - loss: 3392838545.8605 - acc: 0.0000e+00 - val_loss: 1998993734.2439 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "817/817 [==============================] - 0s 504us/step - loss: 2798575071.1775 - acc: 0.0000e+00 - val_loss: 1541006626.3415 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "817/817 [==============================] - 0s 513us/step - loss: 2331164136.8127 - acc: 0.0000e+00 - val_loss: 1334252848.3902 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "817/817 [==============================] - 0s 548us/step - loss: 2070327074.0759 - acc: 0.0000e+00 - val_loss: 1271517013.0732 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "817/817 [==============================] - 0s 509us/step - loss: 1984294794.8103 - acc: 0.0000e+00 - val_loss: 1249201792.0000 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "817/817 [==============================] - 0s 523us/step - loss: 1896886041.1457 - acc: 0.0000e+00 - val_loss: 1252601888.7805 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "817/817 [==============================] - 0s 513us/step - loss: 1911384852.0147 - acc: 0.0000e+00 - val_loss: 1241692885.8537 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "817/817 [==============================] - 0s 525us/step - loss: 1879321472.2350 - acc: 0.0000e+00 - val_loss: 1238826995.5122 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "817/817 [==============================] - 0s 533us/step - loss: 1860536722.5655 - acc: 0.0000e+00 - val_loss: 1231926453.0732 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "817/817 [==============================] - 0s 524us/step - loss: 1906052192.4308 - acc: 0.0000e+00 - val_loss: 1226146115.1220 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "817/817 [==============================] - 0s 535us/step - loss: 1814173202.6047 - acc: 0.0000e+00 - val_loss: 1236535893.8537 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "817/817 [==============================] - 0s 548us/step - loss: 1812892938.6536 - acc: 0.0000e+00 - val_loss: 1261386474.1463 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "817/817 [==============================] - 0s 548us/step - loss: 1802127424.8617 - acc: 0.0000e+00 - val_loss: 1258477006.0488 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "817/817 [==============================] - 0s 529us/step - loss: 1815386337.0967 - acc: 0.0000e+00 - val_loss: 1393590549.8537 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "817/817 [==============================] - 0s 523us/step - loss: 1783956556.0636 - acc: 0.0000e+00 - val_loss: 1259015344.3902 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "817/817 [==============================] - 0s 511us/step - loss: 1818770292.4064 - acc: 0.0000e+00 - val_loss: 1369861068.4878 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "817/817 [==============================] - 0s 513us/step - loss: 1804498879.4517 - acc: 0.0000e+00 - val_loss: 1221530304.0000 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "817/817 [==============================] - 0s 505us/step - loss: 1775239235.0551 - acc: 0.0000e+00 - val_loss: 1335456388.6829 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "817/817 [==============================] - 0s 508us/step - loss: 1803717307.8482 - acc: 0.0000e+00 - val_loss: 1235563249.1707 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "817/817 [==============================] - 0s 525us/step - loss: 1817457018.0465 - acc: 0.0000e+00 - val_loss: 1645416769.5610 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "817/817 [==============================] - 0s 499us/step - loss: 1791936431.4712 - acc: 0.0000e+00 - val_loss: 1291394527.2195 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "817/817 [==============================] - 0s 507us/step - loss: 1724665064.7344 - acc: 0.0000e+00 - val_loss: 1368581355.7073 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "817/817 [==============================] - 0s 522us/step - loss: 1765036910.8054 - acc: 0.0000e+00 - val_loss: 1243019649.5610 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "817/817 [==============================] - 0s 542us/step - loss: 1738522185.0477 - acc: 0.0000e+00 - val_loss: 1325407850.1463 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "817/817 [==============================] - 0s 579us/step - loss: 1740173761.5863 - acc: 0.0000e+00 - val_loss: 1257984627.5122 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "817/817 [==============================] - 0s 532us/step - loss: 1789412977.2338 - acc: 0.0000e+00 - val_loss: 1226944767.2195 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "817/817 [==============================] - 0s 518us/step - loss: 1808981926.4235 - acc: 0.0000e+00 - val_loss: 1240866449.1707 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "817/817 [==============================] - 0s 495us/step - loss: 1696088366.4137 - acc: 0.0000e+00 - val_loss: 1265882194.7317 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "817/817 [==============================] - 0s 511us/step - loss: 1661228954.7124 - acc: 0.0000e+00 - val_loss: 1260958977.5610 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "817/817 [==============================] - 0s 499us/step - loss: 1660100076.2203 - acc: 0.0000e+00 - val_loss: 1321183500.4878 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "817/817 [==============================] - 0s 495us/step - loss: 1693523957.5814 - acc: 0.0000e+00 - val_loss: 1264125939.5122 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "817/817 [==============================] - 0s 521us/step - loss: 1676842962.3305 - acc: 0.0000e+00 - val_loss: 1273379485.6585 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "817/817 [==============================] - 0s 498us/step - loss: 1763599571.5447 - acc: 0.0000e+00 - val_loss: 1504687324.0976 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "817/817 [==============================] - 0s 510us/step - loss: 1720254974.5508 - acc: 0.0000e+00 - val_loss: 1269880020.2927 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "817/817 [==============================] - 0s 519us/step - loss: 1639227562.1640 - acc: 0.0000e+00 - val_loss: 1325368760.1951 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "817/817 [==============================] - 0s 551us/step - loss: 1643059427.8384 - acc: 0.0000e+00 - val_loss: 1266145982.4390 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "817/817 [==============================] - 0s 532us/step - loss: 1615785297.3905 - acc: 0.0000e+00 - val_loss: 1255500049.1707 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "817/817 [==============================] - 0s 507us/step - loss: 1659811313.7430 - acc: 0.0000e+00 - val_loss: 1422172768.7805 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "817/817 [==============================] - 0s 512us/step - loss: 1694665501.8458 - acc: 0.0000e+00 - val_loss: 1467760555.7073 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "817/817 [==============================] - 0s 553us/step - loss: 1693184041.7723 - acc: 0.0000e+00 - val_loss: 1320705404.8780 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "817/817 [==============================] - 0s 518us/step - loss: 1590485330.9572 - acc: 0.0000e+00 - val_loss: 1230170514.7317 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "817/817 [==============================] - 0s 548us/step - loss: 1557761578.0661 - acc: 0.0012 - val_loss: 1288183659.7073 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "817/817 [==============================] - 0s 538us/step - loss: 1561960101.2093 - acc: 0.0000e+00 - val_loss: 1218302822.2439 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "817/817 [==============================] - 0s 544us/step - loss: 1592899835.7895 - acc: 0.0000e+00 - val_loss: 1273590645.0732 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "817/817 [==============================] - 0s 545us/step - loss: 1553294711.2656 - acc: 0.0000e+00 - val_loss: 1333743887.6098 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "817/817 [==============================] - 0s 543us/step - loss: 1675811057.3513 - acc: 0.0000e+00 - val_loss: 1255382029.2683 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817/817 [==============================] - 0s 555us/step - loss: 1553783361.9976 - acc: 0.0000e+00 - val_loss: 1364093429.0732 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "817/817 [==============================] - 0s 572us/step - loss: 1616512643.7209 - acc: 0.0000e+00 - val_loss: 1420352032.7805 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "817/817 [==============================] - 0s 554us/step - loss: 1569241661.8458 - acc: 0.0000e+00 - val_loss: 1300533671.0244 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "817/817 [==============================] - 0s 572us/step - loss: 1544270961.5471 - acc: 0.0000e+00 - val_loss: 1246917942.6341 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "817/817 [==============================] - 0s 528us/step - loss: 1588134056.1077 - acc: 0.0000e+00 - val_loss: 1314397661.6585 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "817/817 [==============================] - 0s 555us/step - loss: 1513047762.8397 - acc: 0.0000e+00 - val_loss: 1306629135.6098 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "817/817 [==============================] - 0s 536us/step - loss: 1501502509.3562 - acc: 0.0000e+00 - val_loss: 1321847043.1220 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "817/817 [==============================] - 0s 492us/step - loss: 1595232823.2166 - acc: 0.0000e+00 - val_loss: 1254621586.7317 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "817/817 [==============================] - 0s 529us/step - loss: 1582677502.9816 - acc: 0.0000e+00 - val_loss: 1471298897.1707 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "817/817 [==============================] - 0s 488us/step - loss: 1464751555.0942 - acc: 0.0000e+00 - val_loss: 1342227451.3171 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "817/817 [==============================] - 0s 537us/step - loss: 1472826080.5875 - acc: 0.0000e+00 - val_loss: 1257368053.0732 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "817/817 [==============================] - 0s 508us/step - loss: 1546129343.8629 - acc: 0.0000e+00 - val_loss: 1275406593.5610 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "817/817 [==============================] - 0s 493us/step - loss: 1464360866.1934 - acc: 0.0000e+00 - val_loss: 1310407966.4390 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "817/817 [==============================] - 0s 506us/step - loss: 1432250332.1812 - acc: 0.0000e+00 - val_loss: 1316929102.0488 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "817/817 [==============================] - 0s 519us/step - loss: 1479735909.9927 - acc: 0.0012 - val_loss: 1423054078.4390 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "817/817 [==============================] - 0s 580us/step - loss: 1474725560.0294 - acc: 0.0000e+00 - val_loss: 1395267776.0000 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "817/817 [==============================] - 0s 511us/step - loss: 1476256360.0685 - acc: 0.0000e+00 - val_loss: 1431805644.4878 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "817/817 [==============================] - 0s 540us/step - loss: 1444104224.0783 - acc: 0.0000e+00 - val_loss: 1274838499.1220 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "817/817 [==============================] - 0s 536us/step - loss: 1504930864.3721 - acc: 0.0000e+00 - val_loss: 1277702348.4878 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "817/817 [==============================] - 0s 506us/step - loss: 1424424534.0514 - acc: 0.0000e+00 - val_loss: 1278713916.8780 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "817/817 [==============================] - 0s 482us/step - loss: 1447811729.7038 - acc: 0.0000e+00 - val_loss: 1291141692.8780 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "817/817 [==============================] - 0s 488us/step - loss: 1527342402.5851 - acc: 0.0000e+00 - val_loss: 1280896210.7317 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "817/817 [==============================] - 0s 512us/step - loss: 1450452467.9364 - acc: 0.0000e+00 - val_loss: 1331373775.6098 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "817/817 [==============================] - 0s 497us/step - loss: 1436249351.5985 - acc: 0.0000e+00 - val_loss: 1273070687.2195 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "817/817 [==============================] - 0s 515us/step - loss: 1400946015.4908 - acc: 0.0000e+00 - val_loss: 1389541294.8293 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "817/817 [==============================] - 0s 501us/step - loss: 1389148620.9253 - acc: 0.0000e+00 - val_loss: 1265481820.8780 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "817/817 [==============================] - 0s 506us/step - loss: 1366726908.4553 - acc: 0.0000e+00 - val_loss: 1298529602.3415 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "817/817 [==============================] - 0s 502us/step - loss: 1388240766.5900 - acc: 0.0000e+00 - val_loss: 1298702234.5366 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "817/817 [==============================] - 0s 505us/step - loss: 1400416469.6989 - acc: 0.0000e+00 - val_loss: 1272715178.1463 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "817/817 [==============================] - 0s 509us/step - loss: 1396120833.2925 - acc: 0.0000e+00 - val_loss: 1382062059.7073 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "817/817 [==============================] - 0s 496us/step - loss: 1400311866.3599 - acc: 0.0000e+00 - val_loss: 1329825141.0732 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "817/817 [==============================] - 0s 511us/step - loss: 1421891283.2705 - acc: 0.0000e+00 - val_loss: 1528140278.6341 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "817/817 [==============================] - 0s 501us/step - loss: 1357317463.5202 - acc: 0.0000e+00 - val_loss: 1311158407.8049 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "817/817 [==============================] - 0s 504us/step - loss: 1406748629.2876 - acc: 0.0000e+00 - val_loss: 1329271520.7805 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "817/817 [==============================] - 0s 497us/step - loss: 1393040318.4333 - acc: 0.0000e+00 - val_loss: 1466719576.9756 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "817/817 [==============================] - 0s 525us/step - loss: 1457248332.6512 - acc: 0.0000e+00 - val_loss: 1327541544.5854 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "817/817 [==============================] - 0s 540us/step - loss: 1403059292.7491 - acc: 0.0000e+00 - val_loss: 1301092231.8049 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "817/817 [==============================] - 0s 527us/step - loss: 1402463159.7748 - acc: 0.0000e+00 - val_loss: 1278021098.1463 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "817/817 [==============================] - 0s 535us/step - loss: 1393371593.4002 - acc: 0.0000e+00 - val_loss: 1332320215.4146 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "817/817 [==============================] - 0s 486us/step - loss: 1341213559.2656 - acc: 0.0000e+00 - val_loss: 1296795616.7805 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "817/817 [==============================] - 0s 506us/step - loss: 1315003407.2362 - acc: 0.0000e+00 - val_loss: 1336007366.2439 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "817/817 [==============================] - 0s 543us/step - loss: 1367206340.6610 - acc: 0.0000e+00 - val_loss: 1398671085.2683 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "817/817 [==============================] - 0s 506us/step - loss: 1395809558.4039 - acc: 0.0000e+00 - val_loss: 1479869443.1220 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "817/817 [==============================] - 0s 556us/step - loss: 1364507376.4113 - acc: 0.0000e+00 - val_loss: 1312426953.3659 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "817/817 [==============================] - 0s 510us/step - loss: 1266035486.0416 - acc: 0.0000e+00 - val_loss: 1436469151.2195 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "817/817 [==============================] - 0s 544us/step - loss: 1334628586.9670 - acc: 0.0000e+00 - val_loss: 1444258681.7561 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "817/817 [==============================] - 0s 531us/step - loss: 1352676953.3023 - acc: 0.0000e+00 - val_loss: 1293947573.0732 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "817/817 [==============================] - 0s 550us/step - loss: 1365829934.4333 - acc: 0.0000e+00 - val_loss: 1434098528.7805 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "817/817 [==============================] - 0s 533us/step - loss: 1321200935.8727 - acc: 0.0000e+00 - val_loss: 1375579324.8780 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "817/817 [==============================] - 0s 552us/step - loss: 1347441155.8776 - acc: 0.0000e+00 - val_loss: 1396338683.3171 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "history =modelNN.fit(X_train,Y_train,validation_split=0.20,epochs=100, batch_size=10)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
